{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce35f8-5a2d-4b0c-a1b4-9572aa984c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn tqdm tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d410979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7179f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TAGS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc57669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_idx(filename):\n",
    "    return int(filename.split('/')[-1].split('.')[0])\n",
    "\n",
    "def load_and_count_embeds(fn):\n",
    "    embeds = np.load(fn)\n",
    "    track_idx = get_track_idx(fn)\n",
    "    return track_idx, embeds.shape[0], embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9578fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_tags, num_layers=8, nhead=8, dim_feedforward=2048):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(input_dim, num_tags)\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.LayerNorm(input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = src.permute(1, 0, 2)\n",
    "        if src_mask is not None:\n",
    "            src_key_padding_mask = ~src_mask\n",
    "        else:\n",
    "            src_key_padding_mask = None\n",
    "        transformer_output = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.fc(self.lin(transformer_output[-1]))\n",
    "        return output\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * ((1 - pt) ** self.gamma) * BCE_loss\n",
    "        return F_loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c52371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alpha_gamma(df, num_tags):\n",
    "    label_counts = np.zeros(num_tags)\n",
    "    for tags in df['tags']:\n",
    "        indices = list(map(int, tags.split(',')))\n",
    "        label_counts[indices] += 1\n",
    "\n",
    "    alpha = 1 - (label_counts / np.sum(label_counts))\n",
    "    gamma = 2 * np.ones(num_tags)\n",
    "\n",
    "    return torch.tensor(alpha, dtype=torch.float32), torch.tensor(gamma, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c6d8e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df_train = pd.read_csv('train.csv')\n",
    "    df_test = pd.read_csv('test.csv')\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071b6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, track_idx2embeds):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, row in df.iterrows():\n",
    "        track_idx = row['track']\n",
    "        embed = np.array(track_idx2embeds[track_idx], dtype=np.float32)\n",
    "        tags = list(map(int, row['tags'].split(','))) if pd.notnull(row['tags']) else []\n",
    "        y = np.zeros(NUM_TAGS)\n",
    "        y[tags] = 1\n",
    "        X.append(torch.tensor(embed, dtype=torch.float32))\n",
    "        Y.append(torch.tensor(y, dtype=torch.float32))\n",
    "    return list(zip(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b638a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    lengths = torch.tensor([x.size(0) for x in x_batch], dtype=torch.long)\n",
    "    x_padded = pad_sequence(x_batch, batch_first=True)\n",
    "    y_padded = pad_sequence(y_batch, batch_first=True, padding_value=0)\n",
    "\n",
    "    mask = torch.zeros(x_padded.size(0), x_padded.size(1), dtype=torch.bool)\n",
    "    for i, length in enumerate(lengths):\n",
    "        mask[i, :length] = 1\n",
    "\n",
    "    return x_padded, y_padded, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a05bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"checkpoint.pth\"):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    if os.path.isfile(filename):\n",
    "        return torch.load(filename)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "475d0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, writer, start_epoch, device, checkpoint_path=None):\n",
    "    step = 0\n",
    "    for epoch in tqdm(range(start_epoch, 5), desc=\"Epoch\"):  # Assuming 10 is the total number of epochs\n",
    "        model.train()\n",
    "        pbar = tqdm(enumerate(train_loader), desc=f\"Training Epoch {epoch}\", total=len(train_loader))\n",
    "        for i, (x, y, mask) in pbar:\n",
    "            x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x, mask)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + i)\n",
    "            pbar.set_postfix({'loss' : loss.item()})\n",
    "            step += 1\n",
    "\n",
    "            # Save checkpoint every 100 steps\n",
    "            if step % 500 == 0:\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                }, filename=checkpoint_path if checkpoint_path is not None else f\"checkpoint{i}.pth\")\n",
    "\n",
    "        validate_model(model, val_loader, criterion, writer, epoch, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d859731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def validate_model(model, val_loader, criterion, writer, epoch, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_average_precisions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, mask in tqdm(val_loader, desc=\"Validating\"):\n",
    "            x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "            outputs = model(x, mask)\n",
    "            loss = criterion(outputs, y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate average precision score\n",
    "            y_true = y.cpu().detach().numpy()\n",
    "            y_pred = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "            average_precision = average_precision_score(y_true, y_pred)\n",
    "            all_average_precisions.append(average_precision)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_precision = sum(all_average_precisions) / len(all_average_precisions)\n",
    "\n",
    "    writer.add_scalar('Loss/val', avg_val_loss, epoch)\n",
    "    writer.add_scalar('AveragePrecision/val', avg_precision, epoch)\n",
    "    print(f\"{avg_val_loss=}, {avg_precision=}\")\n",
    "    return avg_val_loss, avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62c51192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, df_test, track_idx2embeds, checkpoint_path=None):\n",
    "    if checkpoint_path:\n",
    "        checkpoint = load_checkpoint(checkpoint_path)\n",
    "        if checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    predictions_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for index, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Generating predictions\", leave=True):\n",
    "            track_idx = row['track']\n",
    "            embed = np.array(track_idx2embeds[track_idx], dtype=np.float32)\n",
    "            x = torch.tensor(embed, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            outputs = model(x)\n",
    "            predictions = torch.sigmoid(outputs).squeeze().tolist()\n",
    "            predictions_list.append([track_idx, \",\".join(map(str, predictions))])\n",
    "    return predictions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d5d681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "df_train, df_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d8a5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "# Convert tags to a binary matrix\n",
    "Y = np.zeros((df_train.shape[0], NUM_TAGS), dtype=int)\n",
    "for idx, tags in enumerate(df_train['tags']):\n",
    "    indices = list(map(int, tags.split(',')))\n",
    "    Y[idx, indices] = 1\n",
    "\n",
    "# Splitting the data\n",
    "X_train, y_train, X_val, y_val = iterative_train_test_split(df_train.values, Y, test_size=0.1)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_train = pd.DataFrame(X_train, columns=df_train.columns)\n",
    "df_train['tags'] = [','.join(map(str, np.where(row == 1)[0])) for row in y_train]\n",
    "\n",
    "df_val = pd.DataFrame(X_val, columns=df_train.columns)\n",
    "df_val['tags'] = [','.join(map(str, np.where(row == 1)[0])) for row in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "832db03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, gamma = calculate_alpha_gamma(df_train, NUM_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "043d9b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4534940f5b8f4288a01c6409f517c7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_list = glob.glob('track_embeddings/*.npy')\n",
    "track_idx2embeds = {}\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for track_idx, num_embeds, embeds in tqdm(executor.map(load_and_count_embeds, file_list), total=len(file_list)):\n",
    "        track_idx2embeds[track_idx] = embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aacdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data(df_train, track_idx2embeds)\n",
    "val_data = prepare_data(df_val, track_idx2embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25937023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_data, batch_size=128, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a63250ca-a3d4-4103-bba8-02954acd9dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(input_dim=768, num_tags=NUM_TAGS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccf0503f-dfad-4ecc-9d61-2a9af2572bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "criterion = FocalLoss(alpha=alpha.to(device), gamma=gamma.to(device))\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58c62f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = None #\"checkpoint.pth\"\n",
    "checkpoint = load_checkpoint(checkpoint_path) if checkpoint_path is not None else None\n",
    "\n",
    "if checkpoint is not None:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    step = checkpoint['step']\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00b9ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") # shutup sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ee0881c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54b8917e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e3dbbcd63142f0b999ae6ab5244c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa90caac96a431da28b12bc332294ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19b8d8d334f42b6b5710ba1eb836482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_val_loss=430.11662999595086, avg_precision=0.2477226595272815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2d8afbf2aa4ff3a0807b2e6f21e568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00abad2954fa46f58d26674a2b87d60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_val_loss=425.66731150557354, avg_precision=0.24978967571473462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128dd0773d684398975369424744d8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2862c580eb1f4572930435b71ab9cc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_val_loss=422.8435694996904, avg_precision=0.25323879912449315\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a35c55347da4e3fbff978447e87660c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbae73e53304b198d6e6f8d648e5062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_val_loss=422.12120316668256, avg_precision=0.25516544807983904\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb224a0978b4e889a548dd30862538c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae9dd01b3de404ea50c3d5c70e86070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_val_loss=417.11289978027344, avg_precision=0.2590898119911534\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63f172b8c8141d78dfa2a8e85e30ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_val_loss=417.11289978027344, avg_precision=0.2590898119911534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(417.11289978027344, 0.2590898119911534)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, train_loader, optimizer, criterion, writer, start_epoch, device)\n",
    "validate_model(model, val_loader, criterion, writer, start_epoch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d990c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85e0f662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9a7373c24246539ef171ee8eddc475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating predictions:   0%|          | 0/25580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_list = generate_predictions(model, df_test, track_idx2embeds, checkpoint_path)\n",
    "df_predictions = pd.DataFrame(predictions_list, columns=[\"track\", \"prediction\"])\n",
    "df_predictions.to_csv(\"prediction_09_40_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602df22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
